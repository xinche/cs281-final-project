{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Milestone 4: Deep learning, due Wednesday, April 26, 2017\n",
    "\n",
    "For this milestone you will (finally) use deep learning to predict movie genres. You will train one small network from scratch on the posters only, and compare this one to a pre-trained network that you fine tune. [Here](https://keras.io/getting-started/faq/#how-can-i-use-pre-trained-models-in-keras) is a description of how to use pretrained models in Keras.\n",
    "\n",
    "You can try different architectures, initializations, parameter settings, optimization methods, etc. Be adventurous and explore deep learning! It can be fun to combine the features learned by the deep learning model with a SVM, or incorporate meta data into your deep learning model. \n",
    "\n",
    "**Note:** Be mindful of the longer training times for deep models. Not only for training time, but also for the parameter tuning efforts. You need time to develop a feel for the different parameters and which settings work, which normalization you want to use, which model architecture you choose, etc. \n",
    "\n",
    "It is great that we have GPUs via AWS to speed up the actual computation time, but you need to be mindful of your AWS credits. The GPU instances are not cheap and can accumulate costs rather quickly. Think about your model first and do some quick dry runs with a larger learning rate or large batch size on your local machine. \n",
    "\n",
    "The notebook to submit this week should at least include:\n",
    "\n",
    "- Complete description of the deep network you trained from scratch, including parameter settings, performance, features learned, etc. \n",
    "- Complete description of the pre-trained network that you fine tuned, including parameter settings, performance, features learned, etc. \n",
    "- Discussion of the results, how much improvement you gained with fine tuning, etc. \n",
    "- Discussion of at least one additional exploratory idea you pursued"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Xincheng/anaconda/lib/python2.7/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import urllib\n",
    "import cStringIO\n",
    "from PIL import Image\n",
    "from imdb import IMDb\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import Series, DataFrame\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import time\n",
    "import ast\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.cross_validation import KFold\n",
    "import difflib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sing ['Animation', 'Comedy', 'Drama', 'Family', 'Music']\n",
      "Split ['Horror', 'Thriller']\n",
      "Fantastic Beasts and Where to Find Them ['Action', 'Adventure', 'Fantasy']\n",
      "Rogue One: A Star Wars Story ['Action', 'Drama', 'Science Fiction', 'War']\n",
      "Deadpool ['Action', 'Adventure', 'Comedy', 'Romance']\n",
      "Arrival ['Thriller', 'Drama', 'Science Fiction', 'Mystery']\n",
      "Boyka: Undisputed IV ['Action']\n",
      "La La Land ['Comedy', 'Drama', 'Music', 'Romance']\n",
      "Doctor Strange ['Action', 'Adventure', 'Fantasy', 'Science Fiction']\n",
      "Tomorrow Everything Starts ['Drama', 'Comedy']\n",
      "Captain America: Civil War ['Adventure', 'Action', 'Science Fiction']\n",
      "Finding Dory ['Adventure', 'Animation', 'Comedy', 'Family']\n",
      "Collateral Beauty ['Drama', 'Romance']\n",
      "X-Men: Apocalypse ['Action', 'Adventure', 'Fantasy', 'Science Fiction']\n",
      "Passengers ['Adventure', 'Drama', 'Romance', 'Science Fiction']\n",
      "Why Him? ['Comedy']\n",
      "Underworld: Blood Wars ['Action', 'Horror']\n",
      "Suicide Squad ['Action', 'Crime', 'Fantasy', 'Science Fiction']\n",
      "Hacksaw Ridge ['Drama', 'History', 'War']\n",
      "Assassin's Creed ['Action', 'Adventure', 'Fantasy', 'Science Fiction']\n"
     ]
    }
   ],
   "source": [
    "# part 3 - top 10 most popular movies of 2016 from TMDb and their genre\n",
    "top_2016_1 = urllib.urlopen(\"https://api.themoviedb.org/3/discover/movie?api_key=2dc6c9f1d17bd39dcbaef83321e1b5a3&sort_by=popularity.desc&include_adult=false&include_video=false&page=1&primary_release_year=2016\")\n",
    "top_2016_1_json = json.loads(top_2016_1.read())\n",
    "\n",
    "# get genre list\n",
    "genre_list = urllib.urlopen(\"https://api.themoviedb.org/3/genre/movie/list?api_key=2dc6c9f1d17bd39dcbaef83321e1b5a3&language=en-US\")\n",
    "\n",
    "genre_list_json = json.loads(genre_list.read()) \n",
    "\n",
    "genre_lst = {}\n",
    "for i in genre_list_json['genres']:\n",
    "    genre_lst[i['id']] = str(i['name'])\n",
    "    \n",
    "# top most popular movies of 2016\n",
    "top_2016_1 = urllib.urlopen(\"https://api.themoviedb.org/3/discover/movie?api_key=2dc6c9f1d17bd39dcbaef83321e1b5a3&sort_by=popularity.desc&include_adult=false&include_video=false&page=1&primary_release_year=2016\")\n",
    "top_2016_1_json = json.loads(top_2016_1.read())\n",
    "\n",
    "\n",
    "for i in top_2016_1_json['results']:\n",
    "    print i['title'], [genre_lst[j] for j in i['genre_ids']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import ast\n",
    "\n",
    "movie_2000_df = pd.read_csv('tmdb_metadata.csv')\n",
    "movie_2000_df = movie_2000_df.drop('Unnamed: 0', axis=1)\n",
    "\n",
    "movie_2000_df = movie_2000_df.dropna()\n",
    "\n",
    "labels = []\n",
    "for i in movie_2000_df.genre_ids:\n",
    "    label_matrix = np.zeros(len(genre_lst.keys()), dtype=int)\n",
    "    for j in ast.literal_eval(i):\n",
    "        if j in genre_lst.keys():\n",
    "            label_matrix[genre_lst.keys().index(j)] = 1\n",
    "    labels.append(label_matrix)\n",
    "movie_2000_df['labels'] = labels\n",
    "\n",
    "# convert dates\n",
    "import datetime\n",
    "def to_integer(dt_time):\n",
    "    return 10000*dt_time.year + 100*dt_time.month + dt_time.day\n",
    "\n",
    "int_dates =[]\n",
    "\n",
    "for i in movie_2000_df.release_date:\n",
    "    f = i.split('-')\n",
    "    a = datetime.date(int(f[0]), int(f[1]), int(f[2]))\n",
    "    int_dates.append(to_integer(a))\n",
    "\n",
    "movie_2000_df['int_dates'] = int_dates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = movie_2000_df.drop(['genre_ids', 'movie_id', 'poster_path', 'overview', 'title', 'release_date'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "words = pd.read_csv('genre_words_pca.csv').drop('Unnamed: 0', axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "x = pd.concat([data[['popularity', 'vote_average', 'vote_count', 'int_dates']], words], axis = 1).values\n",
    "y = data['labels']\n",
    "y = np.asarray(y.tolist())\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size = 0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "\n",
    "import keras\n",
    "from keras.datasets import mnist\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "import matplotlib\n",
    "sns.set_style('white')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3431 train samples\n",
      "1471 test samples\n"
     ]
    }
   ],
   "source": [
    "# smaller batch size means noisier gradient, but more updates per epoch\n",
    "batch_size = 512\n",
    "# this is fixed, we have 10 digits in our data set\n",
    "num_classes = 10\n",
    "# number of iterations over the complete training data\n",
    "epochs = 100\n",
    "\n",
    "# the data, shuffled and split between train and test sets\n",
    "# (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "# x_train = x_train.reshape(60000, 784)\n",
    "# x_test = x_test.reshape(10000, 784)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "# normalize image values to [0,1]\n",
    "# interestingly the keras example code does not center the data\n",
    "# x_train /= 255\n",
    "# x_test /= 255\n",
    "print(x_train.shape[0], 'train samples')\n",
    "print(x_test.shape[0], 'test samples')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "dense_1 (Dense)              (None, 64)                19520     \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 64)                4160      \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 19)                1235      \n",
      "=================================================================\n",
      "Total params: 24,915\n",
      "Trainable params: 24,915\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# create an empty network model\n",
    "model = Sequential()\n",
    "# add an input layer\n",
    "model.add(Dense(64, activation='relu', input_shape=(304,)))\n",
    "# this is our hidden layer\n",
    "model.add(Dense(64, activation='relu'))\n",
    "# and an output layer\n",
    "# note that the 10 is the number of classes we have\n",
    "# the classes are mutually exclusive so softmax is a good choice\n",
    "model.add(Dense(19, activation='sigmoid'))\n",
    "\n",
    "# prints out a summary of the model architecture\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import metrics\n",
    "import keras.backend as K\n",
    "\n",
    "def precision(y_true, y_pred):\n",
    "    \"\"\"Precision metric.\n",
    "    Only computes a batch-wise average of precision.\n",
    "    Computes the precision, a metric for multi-label classification of\n",
    "    how many selected items are relevant.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "\n",
    "def recall(y_true, y_pred):\n",
    "    \"\"\"Recall metric.\n",
    "    Only computes a batch-wise average of recall.\n",
    "    Computes the recall, a metric for multi-label classification of\n",
    "    how many relevant items are selected.\n",
    "    \"\"\"\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "\n",
    "    # Count positive samples.\n",
    "    c1 = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    c2 = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    c3 = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "\n",
    "    # If there are no true samples, fix the F1 score at 0.\n",
    "    if c3 == 0:\n",
    "        return 0\n",
    "\n",
    "    # How many selected items are relevant?\n",
    "    precision = c1 / c2\n",
    "\n",
    "    # How many relevant items are selected?\n",
    "    recall = c1 / c3\n",
    "\n",
    "    # Calculate f1_score\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall)\n",
    "    return f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sgd = SGD(lr=0.01, momentum=0.9)\n",
    "model.compile(loss='binary_crossentropy',\n",
    "              optimizer=sgd,\n",
    "              metrics=['accuracy', precision, recall, f1_score])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 3431 samples, validate on 1471 samples\n",
      "Epoch 1/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 2/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6050 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 3/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 4/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 5/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 6/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 7/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 8/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 9/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 10/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 11/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 12/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 13/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 14/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 15/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 16/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6052 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 17/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 18/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 19/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 20/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 21/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 22/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 23/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6051 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 24/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 25/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 26/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 27/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 28/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 29/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 30/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 31/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 32/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6052 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 33/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 34/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 35/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 36/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 37/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 38/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 39/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 40/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 41/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 42/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 43/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 44/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 45/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 46/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 47/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 48/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 49/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 50/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 51/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 52/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 53/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 54/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 55/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 56/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 57/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 58/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 59/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 60/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 61/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 62/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 63/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 64/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 65/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 66/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 67/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 68/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 69/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 70/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 71/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 72/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 73/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6052 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 74/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 75/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 76/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 77/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6056 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 78/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 79/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6052 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 80/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 81/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 82/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 83/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 84/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 85/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 86/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6052 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 87/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 88/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 89/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 90/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 91/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 92/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 93/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6051 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 94/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6055 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 95/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 96/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 97/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 98/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6054 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 99/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6051 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Epoch 100/100\n",
      "3431/3431 [==============================] - 0s - loss: 10.4991 - acc: 0.3420 - precision: 0.1099 - recall: 0.6053 - f1_score: 0.1860 - val_loss: 10.5375 - val_acc: 0.3396 - val_precision: 0.1082 - val_recall: 0.5957 - val_f1_score: 0.1831\n",
      "Test loss: 10.5374518893\n",
      "Test accuracy: 0.339582815635\n",
      "Test precision: 0.108194321323\n",
      "Test recall: 0.594176930647\n",
      "Test f1_score: 0.182891872203\n"
     ]
    }
   ],
   "source": [
    "# this is not the actual training\n",
    "# in addition to the training data we provide validation data\n",
    "# this data is used to calculate the performance of the model over all the epochs\n",
    "# this is useful to determine when training should stop\n",
    "# in our case we just use it to monitor the evolution of the model over the training epochs\n",
    "# if we use the validation data to determine when to stop the training or which model to save, we \n",
    "# should not use the test data, but a separate validation set. \n",
    "history = model.fit(x_train, y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=epochs,\n",
    "                    verbose=1,\n",
    "                    validation_data=(x_test, y_test))\n",
    "\n",
    "# once training is complete, let's see how well we have done\n",
    "score = model.evaluate(x_test, y_test, verbose=0)\n",
    "print('Test loss:', score[0])\n",
    "print('Test accuracy:', score[1])\n",
    "print('Test precision:', score[2])\n",
    "print('Test recall:', score[3])\n",
    "print('Test f1_score:', score[4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
